\documentclass[12pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage[
backend=biber,
style=nature,
sorting=ynt
]{biblatex}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{tikz}
\usepackage{standalone}
\usepackage{svg}
\usepackage{gensymb}
\usepackage{float}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx, animate}
\usepackage{caption, subcaption}
\usepackage{pgffor}
\usepackage{mathtools}
\usepackage{array}
\addbibresource{bibliography.bib}
\usepackage{hyperref,thmtools}
\linespread{1.5}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark*}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{sublemma}{}[theorem]
\newtheorem{conjecture}[theorem]{Conjecture}

% Override ugly default link
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = blue    %Colour of citations
}

\begin{document}

\begin{section}{Introduction}
The modelling of temporal networks is an important task in many real world applications including symptom interactions for mental health, epidemiology, and protein interactions \cite{jordan2020current,contreras2020temporal,lucas2021inferring,jin2009identifying,masuda2013predicting}.
Temporal networks can be seen as dynamical systems: that is, a system in which we have points, in our case nodes in a network, whose states, the edges connecting them, vary dependent in time.
Discovering the underlying equations governing these dynamical systems proves challenging, because changes in network structure are typically observed in the form of discrete jumps from one state to another, that is, an edge may exist between two nodes at one timestep then dissapear at the next. There cannot be an edge that has half decayed.
Here, we proposes a hybrid statistical and deep learning framework that allows us to model temporal networks as continous-time dynamical systems, discover a fitting set of differential equations describing it, and, exploiting that discovery, predict the time evolution of a network.

Differential equations are useful for modelling systems where the state of one variable can effect the trajectories of other variables. We observe this behaviour in temporal networks; nodes' connections within the network can influence the formation and decay of edges between other nodes, for example the phenonmenon of preferential attatchment observed in \cite{newman2001clustering,capocci2006preferential}. With this observation in mind we might wish to draw on the rich mathematical literature of differential equation modelling.

In the common representation of networks as binary-valued adjacency matrices, the events recorded in a temporal sequence of networks correspond to topological events, such as the appearence or the disappearance of link.
Because of the discrete nature of events, directly modelling the temporal networks as dynamical systems would require the model to handle discrete jumps.
The topological nature of temporal networks, and the discontinuous character of their temporal evolution, make it challenging to use differential equations techniques.

Here, we overcome the discretness problem by interpreting networks as Random Dot Product Graphs, a well established statistical model for complex networks, that embeds nodes in a low-dimensional metric space\cite{athreya2017statistical}. In this way we translate the hard problem of modelling discrete events in the space of networks to the easier problem of modelling continous change in the embedding space. Then, we define and use systems of Neural Network Differential Equations (NNDE) to approximate the time evolution of the embedding space, and symbolic regression techniques to discover the functional form of the fitted NNDEs. These functional forms are interpretable (as they read as classic differential equations) and allow to predict forward in time the evolution of the temporal networks.

In this manuscript, we prove that the temporal network prediction problem can be successfully re-interpreted as a dynamical system modelling problem. In particular, we apply our proposed framework to three small example temporal networks with the hope of exploring the limitations and strengths of the proposed framework.
The framework we are introducing is extremely flexible, and our research regarding the optimal structure of the Neural Networks used for the NNDEs is just started.
We are confident that future research can identify more fitting Neural Network structures than the simple one adopted here.
For this reason, we did not yet attempt to benchmark our model against other classic temporal network prediction methods.
As it is completely general, we believe that the framework we are introducing can be usefully applied to areas of medicine, especially protein interaction networks; population dynamics for network ecology; and social network modelling[citation]. In particular, we discuss how specific domain knowledge relative to the prediction scenario can be taken into account, moving from NNDEs to Universal Differential Equations.

\end{section}

\begin{section}{Methodology}
    We are given a sequence of graphs, and our goal is to predict the next graph in the sequence. To do this we read each graph as a Random Dot Product Graph (RDPG); a visual representation of this process can be seen in \autoref{framework illustration}. To obtain the RDPG, we take each network observation \autoref{method nets} and represent them in the form of an adjacency matrix \autoref{method adjacency}. We then use a truncated singular value decomposition to reduce the dimensionality of our adjacency matrices[ref]. This truncation introduces some loss, but can dramatically reduce the size of our problem [cite]. XXXX It should be noted that when we plot our models, we compare them to this embedding and do not take into account this truncation loss.XXXX The rows of the output of the SVD can then be viewed as vector points in a d-dimensional space[ref], where d is the number of singular values we take for our SVD. In this space nodes that are similar, ie, nodes have many common neighbours, will be mapped to similar points in the space.
    \begin{figure}
    \centering
    \begin{subfigure}[c]{1\textwidth}
        \begin{tabular}{llll}
        \begin{subfigure}[c]{0.25\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/methodology1.tex}}
            \label{method net, a}
        \end{subfigure}
        &
        \centering
        \begin{subfigure}[c]{0.25\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/methodology2.tex}}
            \label{method net, b}
        \end{subfigure}
        &
        $\cdots$
        &
        \centering
        \begin{subfigure}[c]{0.25\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/methodology3.tex}}
            \label{method net, c}
        \end{subfigure}
        
        \end{tabular}
        \caption{Example sequence of networks.}
        \label{method nets}
        
    \end{subfigure}
    \begin{subfigure}[c]{1\textwidth}
        \begin{tabular}{llll}
            $\begin{bmatrix}
                \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & \cdot & 1 & \cdot & \cdot & \cdot & \cdot & \cdot\\
                1 & 1 & \cdot & 1 & 1 & \cdot & \cdot & \cdot\\
                1 & \cdot & 1 & \cdot & \cdot & \cdot & \cdot & \cdot\\
                \cdot & \cdot & 1 & \cdot & \cdot & 1 & 1 & \cdot\\
                \cdot & \cdot & \cdot & \cdot & 1 & \cdot & \cdot & \cdot\\
                \cdot & \cdot & \cdot & \cdot & 1 & \cdot & \cdot & 1\\
                \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & \cdot
            \end{bmatrix}$
            &
            $\begin{bmatrix}
                \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & \cdot & 1 & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & 1 & \cdot & 1 & 1 & \cdot & \cdot & \cdot\\
                1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot\\
                \cdot & \cdot & 1 & \cdot & \cdot & 1 & 1 & \cdot\\
                \cdot & \cdot & \cdot & \cdot & 1 & \cdot & \cdot & 1\\
                \cdot & \cdot & \cdot & \cdot & 1 & \cdot & \cdot & 1\\
                \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & \cdot
            \end{bmatrix}$
            &
            $\cdots$
            &
            $\begin{bmatrix}
                \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & \cdot & 1 & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & 1 & \cdot & 1 & \cdot & \cdot & \cdot & \cdot\\
                1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot\\
                \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & \cdot\\
                \cdot & \cdot & \cdot & \cdot & 1 & \cdot & 1 & 1\\
                \cdot & \cdot & \cdot & \cdot & 1 & 1 & \cdot & 1\\
                \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & \cdot
            \end{bmatrix}$
            
        \end{tabular}
        \caption{Sequence of sparce adjacency matrices associated with the networks in \autoref{method nets}.}
        \label{method adjacency}
    \end{subfigure}

        \begin{tabular}{llll}

        \end{tabular}

        \caption{Visual illustration of the proposed framework.}
        \label{framework illustration}
    \end{figure}
    
    We used a Singular Value Decomposition of each adjacency matrix to embed our network in a low dimensional space. To execute the SVD\cite{golub1971singular} we use the Julia package [cite]. This package uses implicitly restarted Lanczos iterations\cite{lehoucq1996deflation} to approximate the eigen vectors and singular values of each of our adjacency matrices. From the "svds" function, we obtain two matrices, $L$ and $R$ where:\\
    \begin{align}
        L&=\hat L \sqrt{\hat \Sigma }\\
        R &= \hat R \sqrt{\hat \Sigma }\\
        A &\approx \hat L \hat \Sigma \hat R'\\
        &\approx LR'
    \end{align}
    $L,\hat L, R, \hat R$ are each real valued $n\times d$ matrices, where the columns of $\hat L$ and $\hat R$ are the first $d$ eigenvectors of $AA'$ and $A'A$ respectively ordered by the size of their associated singular values. $\hat \Sigma$ is a $d\times d$ diagonal matrix whose entries are the singular values associated with each eigenvector. \\
    It is important to note that the SVD is not unique. The distances between points will be the same for every embedding, but the orientation may be different, and so, if the SVD of each time step is not alligned, learning will be effectively impossible [<- show this]. Since our approximation method is an iterative approach, to align each SVD, we can simply pick the same initial vector guess for each [probably worth getting a reference]; this will cause the outputs of the Arpack function to each be aligned with eachother XXX with some requirements of the matrix/initial vec?XXX. Altenatively we can use Procrustes alignmet[cite package] to align the matrices after they have been generated.\\
    The approximation  given by this formula can be interpreted as a graph where the entry $(i,j)$ of the matrix is the probability that an edge exists from $i \rightarrow j$.This approach creates a Random Dot Product Graph\cite{athreya2017statistical}, a class of Latent Position Models \cite{hoff2002latent}. Once we have this embedding of points, we look to model the trajectories of each point. That is, with some input data, point location, distance to neighbours, etc, we want to model the ODE of each point. To do this we make use of the SciML ecosystem of packages in the Julia programming language \cite{SciML_C_Rak}.\\

    % IN THE THESIS MAYBE
    % This can be truncated at an arbitrary number of eigenpairs to form a low dimentional approximation to $A$.
    %     \[
    %     A \approx \hat U \hat \Sigma \hat V'
    %     \]\\
    % Where $\hat U, \hat \Sigma, \hat V$ are the first $d$ columns of $U, \Sigma, V$ \cite{golub1971singular}. $\hat U \sqrt{\hat \Sigma}$ is still associated with the in edges of our nodes, and $\sqrt{\hat \Sigma} \hat V' $ is still associated with the out edges of our nodes, but now we have simplified our system whilst keeping as much information as possible due to our descending order of singular values. These two matrices form our $d$ dimensional embedding.\\


     
    
    
    % KNN Explaination
    As a way of limiting the complexity of our model, we limit the input of the neural network to the euclidean distances between the target node and its $k$ nearest neighbours. The target node is the node that we are looking to model the movement of and stays the same throughout each of the temporal networks. 
    \begin{align}
        %\centering
        \mathbb{P}(i\rightarrow j)&\coloneqq L_i \cdot R_j\\
        \mathbb{P}(\hat{i}\rightarrow j)&\coloneqq L_{\hat{i}} \cdot R_j
    \end{align}
    Where $(i,j)$ are nodes in the network and $\hat{i}$ is another node at the same timestep to $i$. This implies that the distance between these two nodes with respect to node $j$ is given by
    \begin{equation}
        \begin{gathered}
            %\centering
            ||\mathbb{P}(i\rightarrow j)-\mathbb{P}(\hat{i}\rightarrow j)||_2 = 
            ||L_i \cdot R_j-L_{\hat{i}} \cdot R_j||_2\\
            =||(L_i -L_{\hat{i}}) \cdot R_j||_2
        \end{gathered}
    \end{equation}
    If we look at the expectation of this, we see
    \begin{equation}
        \begin{gathered}
            %\centering
            \mathbb{E}_j(||(L_i -L_{\hat{i}}) \cdot R_j||_2) = ||L_i -L_{\hat{i}} \cdot \frac{1}{k}\sum_{j\ne i}R_j||_2
        \end{gathered}
    \end{equation}
    Where $k$ is the number of neighbours we chose for the neural network input. Because the distance between points is discribed by the euclidean distance, we use this as our loss metric.

    % Symbolic Regression Explaination
    With our trained NODE, we have a black box that approximates the system that governs the movement of the target node in the embedded space. To make this more interpretable we then look to approximate this black box into a combination of known functions using symbolic regression[citation (use the one from symreg package)]. For this process we need to define a set of functions which the algorithm will use to generate a tree of functions to find the best approximation to our NODE with. When using this method we usually are required to make two assumptions: that we have paired observations of $y(t), \frac{dy}{dt}$, and that the tree of expressions we will need is shallow. We will train the symbolic regression on the predictions of the neural network. By doing this, we can remove these assumptions\cite{kidger2022neural}.
\end{section}

\begin{section}{Data}

    In this paper we simulate three temporal sequences of undirected graphs. For each sequence we will be modelling the node in red, we will refer to this as the target node. For each sequence we train the model on the first 20 timesteps and compare the predictions for the next 15 timesteps. For clarity, each diagram \autoref{2community train series} \autoref{longTail train series} \autoref{3community train series} has been simplified and illustrates the movement of the target node in each sequence.\\
    
    The 2 community network has communities of size 40 and 50, each of which are fully connected. The target node is initially fully connected to the 50 node community. In our simplified diagram \autoref{2community train series, a} that is represented by the five node community. At each time step one edge is removed between the target node and the larger community and replaced with an edge between the target node and the smaller community \autoref{2community train series, b}. This process is then continued \autoref{2community train series, c} for thirty five timesteps to generate our training and test data.

    Figures obtained from \cite{Emma2023}.
    \begin{figure}[H]
        \centering
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/2community1.tex}}
            \caption{Simplification of the 2 Community network at $t=1$.}
            \label{2community train series, a}
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/2community2.tex}}
            \caption{Simplification of the 2 Community network at $t=2$.}
            \label{2community train series, b}
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/2community3.tex}}
            \caption{Simplification of the 2 Community network at $t=3$.}
            \label{2community train series, c}
        \end{subfigure}
        \caption{First 3 time steps of synthetic 2 community network.}
        \label{2community train series}
    \end{figure}

    In the long tail network \autoref{longTail train series}, we have a long chain with fifty nodes and a fully connected component with forty nodes at one end of the chain. This is used to orient the embedding and model. The target node starts attatched to the node that is attatched to this large componet \autoref{longTail train series, a}. At each time step the target node move one node further down the tail \autoref{longTail train series, b}. Again, this is repeated to generate the training and test data \autoref{longTail train series, c}.
    \begin{figure}[H]
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/longTail1.tex}}
            \caption{Simplification of the long tail network at $t=1$.}
            \label{longTail train series, a}
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/longTail2.tex}}
            \caption{Simplification of the long tail network at $t=2$.}
            \label{longTail train series, b}
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/longTail3.tex}}
            \caption{Simplification of the long tail network at $t=3$.}
            \label{longTail train series, c}
        \end{subfigure}\\
        \caption{First 3 time steps of synthetic long tail network.}
        \label{longTail train series}
    \end{figure}    


    We also simulate a sequence with three communities. The communities have sizes 40, 35, 30 and have 25, 24, and 23 edges to the target node respectively. In \autoref{3community train series} the communities have been simplified to have sizes of five, four, and 3. The target node starts connected to all of these communities with each community having a different number of edges \autoref{3community train series, a}. The community with the fewest edges between the target node will have one edge removed at each timestep \autoref{3community train series, b}, and this process is repeated to create the required number of timesteps \autoref{3community train series, c}.  
    \begin{figure}[H]
        \centering
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/3community1.tex}}
            \caption{Simplification of the 3 Community network at $t=1$.}
            \label{3community train series, a}
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/3community2.tex}}
            \caption{Simplification of the 3 Community network at $t=2$.}
            \label{3community train series, b}            
        \end{subfigure}
        \hfill
        \centering
        \begin{subfigure}[c]{0.3\textwidth}
            \centering
            \resizebox{.6\width}{!}{\input{SynthDataDiagrams/3community3.tex}}
            \caption{Simplification of the 3 Community network at $t=3$.}
            \label{3community train series, c}
        \end{subfigure}
        \caption{First 3 time steps of synthetic 3 community network.}
        \label{3community train series}
    \end{figure}
    The 2 community temporal network was selected to illustrate that this process can be used to model a node changing communities in ideal conditions, and the long tail network was selected because we wanted to present these models with a range of problems. The two community problem is difficult for the neural network to learn because the same set of distances as input need to result in different outputs. The long tail problem is difficult for the embedding because a large portion of the network is along the off diagonals; meaning that, to get an accurate embedding, the dimension of the embedding would need to be close to the length of the tail. The three community problem should be the easiest to solve because it has been dileberately constructed to avoid the problems the other two models face.

\end{section}

\begin{section}{Results and Discussion}
    For each of theses systems, we took embedded the temporal network in two dimensions at each time step. The temporal embeddings were then divided into a training set of 20 and a testing set of 15. The output of the trained neural networks was then used to train a symbolic regression model that could use a simple set of addition, subtraction, division, and multiplication. \\
    In fig \autoref{2community series}, \autoref{longtail series},\autoref{3community series} the green points are the embedded coordinates of the node in each of the communities. Each cluster is one separate community. The orange point is the true coordinate of the embedded target node at each time step. The blue point is our model prediction from the true target node at the first time step. As the time progresses, we see the true node move from one community cluster to the other. This is expected from the embedding; the target node starts as very similar to the first community (as it has many connections with nodes in that community) and as time progresses, it gradually becomes less and less similar to the first community and more similar to the second (as the edges between the target node and the first community are replaced with edges to the second community). As such, we see the target node move towards the second community.
    \begin{subsection}{2 Community}
        \begin{figure}
            \foreach \i in {1,...,15} {%
                \begin{subfigure}[p]{0.3\textwidth}
                    \includegraphics[width=\linewidth]{./Plots/Test Only/2communities/2communities \i.png}
                    \caption{t=\i}
                \end{subfigure}\quad
            }
            \caption{2 Community test series. This series shows the comparison of the neural network model, the symbolic regression model trained on the neural network model, and the true solution of the two community system.}
            \label{2community series}
        \end{figure}
        

        In this case we see the neural network in one timestep jumps to around (-1.75, 0.4) where it remains until the end of the training data. In contrast, the symbolic regression remains relatively close to the true target node until the last time step when the symbolic regression moves away from the target node.\\

        % It is interesting to see the symbolic regression model perform so much more poorly than the neural network model in this system. When we view the training section of the data, we see that there is a downward curve in the neural network predictions towards the end of the training data. It seems that the symbolic regression model picked this up and continued the downward trajectory throughout the test data. This downward curve may be because the problem that we are trying to solve is symmetrical. That is the same set of distances as input into the neural network, the target node needs to move away from its nearest neighbours in the first half, then move towards its nearest neighbours in the second half. Potential ways of breaking this symmetry may be to include information of the previous timestep, or to include the absolute position of the target node as input into the neural network.
    \end{subsection}

    \begin{subsection}{Long Tail}
        \begin{figure} 
            \foreach \i in {1,...,15} {%
                \begin{subfigure}[p]{0.3\textwidth}
                    \includegraphics[width=\linewidth]{./Plots/Test Only/longTail/longTail \i.png}
                    \caption{t=\i}
                \end{subfigure}\quad
            }
            \caption{Long Tail test series. This series shows the comparison of the neural network model, the symbolic regression model trained on the neural network model, and the true solution of the long tail system.}
            \label{longtail series}
        \end{figure}
        In fig \autoref{longtail series} we see the true embedding of the target node jump from one arm to the other at each timestep. It is clear that neither the the symbolic regression model nor the neural network model capture this movement. We also see that the symbolic regression model maintains a relatively stable position when compared to the neural network model.

        We see that neither the neural network model capture the movement of the target node to any real extent. One reason for this may again be an issue of symmetrical input. The distances from the k nearest neighbours to the target node do not seem to change much as the target node jumps from one arm to the other, but the neural network somehow needs to learn to jump left then right at alternating timesteps. The solutions to this would be the same as for the two community system.
    \end{subsection}

    \begin{subsection}{Three Community (Please Review)}
        \begin{figure}
            \foreach \i in {1,...,15} {%
                \begin{subfigure}[p]{0.3\textwidth}
                    \includegraphics[width=\linewidth]{./Plots/Test Only/3community/3community \i.png}
                    \caption{t=\i}
                \end{subfigure}\quad
            }
            \caption{3 community test series. This series shows the comparison of the neural network model, the symbolic regression model trained on the neural network model, and the true solution of the 3 community system.}
            \label{3community series}
        \end{figure}
        In fig \autoref{3community series} we again, as the edges between the community with the fewest edges are removed, we see the target node move away from it in the embedding and towards the other two clusters of points (communities).

        % In the neural network only model, we see the prediction follow the true target node very closely at first. We then see the node drift further and further from the true target node. In contrast, the symbolic regression model remains much closer to the true position of the target node as the test data progresses. We see the symbolic regression move slightly ahead of the true location at first, but this distance remains relatively constant thereafter.

        In the neural network model, we see the prediction remain at close to its initial location for the duration of the test. This seems to indicate an attractor in the model.

        In contrast, we see that the symbolic regression model quickly moves ahead of the true location where it remains for the duration of the test.

        % This was by far the largest improvement between the neural network and the symbolic regression. This could be because this system had the least difference between the training predictions and the true position of the target node. At the end of the test data we see the true target node move towards the origin. This is because it now only has edges to one community and so at each timestep it removes one of these. If this had continued, the target node would have no edges. Nodes with no edges are mapped to the origin. When this happened, the symbolic regression model prediction began to move further from the true node.

        \subsection{Network Prediction}
        \begin{figure}
            \begin{center}
                \begin{tabular}{| m{0.25\textwidth} | m{0.25\textwidth} | m{0.25\textwidth} | m{0.25\textwidth} |}
                 \hline
                 Sequence & Embedding Mean Loss & Neural Network Prediction Mean Loss & Symbolic Regression Prediction Mean Loss\\ 
                 \hline
                 \hline
                 2 Community & cell5 & cell6 & c\\ 
                 \hline 
                 Long tail & cell8 & cell9 & c \\ 
                 \hline 
                 3 Community & cell8 & cell9 & c \\ 
                 \hline 
                \end{tabular}
                \end{center}
                \caption{my caption}
        \end{figure}
        \subsection{Summary}
        We see a large difference in the training predictions between the 2 and 3 community systems. In the 2 community system, even in the training period, the predictions tended to wander and be less accurate. Whereas the 3 community training predictions remained very close to the true target node. The movement of the target node in the embedding was very similar between the two systems, and so the difference seems to be that the 2 community has symmetrical inputs.

        These were trained on a relatively small neural network (one hidden layer of 64 nodes), when training this small network one of the issues we encountered was that the weights would become stuck in local minima. To manage this we found that simulate annaeling was helpful. When training on a larger network (4 hidden layers (64,8,8,8)) this does not seem to be as much of a problem.
    \end{subsection}

\end{section}

\begin{section}{Conclusion}
    In this paper, we proposed a novel framework for modelling temporal networks. We then tested this framework on three types of small, synthetic network sequences. Throughout these tests the neural network performed XXXXXXX, with the XXXXXX sequence being especially notable due to XXXXX. These results were likely because XXXXX. In contrast, when we used symbolic regression to approximate the neural network model, we saw that XXXXX.
\end{section} 



\printbibliography

\end{document} 

% draft for miguel
% all absolute pos of nodes
% polar positions

% questions for Giulio:
%     Why both NN and sym reg not just sym reg
%     Why cos dist 