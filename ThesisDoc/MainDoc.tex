\documentclass[12pt]{article}

\usepackage[
backend=biber,
style=nature,
sorting=ynt
]{biblatex}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

\addbibresource{bibliography.bib}


\begin{document}
\begin{titlepage}
   \vspace*{\stretch{1.0}}
   \begin{center}
      \Large\textbf{My Thesis}\\
      \large\textit{Connor Smith}
   \end{center}
   \vspace*{\stretch{2.0}}
\end{titlepage}
\section{Abstract}
About 200-300 words long\\

\section{Introduction}
%\cite{metaInfo}
A bit of background about the topic.
Some information about the current knowledge.
The aim of your research (the gap in knowledge that prompted you to write the thesis).
    \subsection{Terminology?}
        
\section{Literature Review}
    \subsection{Traditional Online Social Network Modelling}
        There has been a lot of research into what governs how edges are formed in an online social network. Some examples of what affects edge formation are a nodes' "age", how much time has passed since it first made an edge; it's edge creation rate is highest shortly after joining and decreases over time. The majority of edge creation in the early stages of a network is driven by new nodes arriving in the network, however this decreases significantly as the network matures. Edges formation follows preferential attachment, but this strength decreases over time as the network expands \cite{zhao2012multi}. Research also indicates that, while preferential attachment (a node is most likely to form an edge with the highest in-degree) can be used to model edge formation in online social networks, it alone does not seem to account for the low number of hops that empirical data suggests edges usually form along. \cite{garg2009evolution} suggest that preferential attachment with node distance as a tie breaker results in a distribution of node distances close to what was seen empirically from  their data.\\
        
        A potential for improvement in this research [\cite{garg2009evolution}, \cite{zhao2012multi}] is that they did not model the decay of edges (to the best of my knowledge) and they used a friendship/follow as their edge, which, while may be meaningful in the short term, I don't think is always meaningful after a given amount of time. [find source] A person may add a friend and never speak to them again (hence we use conversations together), this has limitations where a person can't always choose who they talk to, but it might provide a more granular (definition?) snapshot of interactions between users.\\
    \subsection{UDEs and NODEs}
      \subsubsection{Theory}
         UDEs are a combination of a known, structural component of a system, and a universal approximator, often a neural network.[Find reference]
         Given the flexibility of neural networks, the use of UDEs is natural when modelling a process that is not well understood [On Neural ODEs]
         \\
         Largely, the SciML ecosystem has been optimised for flexibility and efficiency with respect to models available 
         and training performance. Given there does not seem to be any other ecosystem with this feature, it seemed like 
         an obvious choice. ~~~What makes it good (Examples from \cite{SciML_C_Rak})  \\
        
        \subsubsection{Applications [Caution only glanced at these papers]}
        Whilst there has been much interest in implementing UDEs and neural ODEs, much of this work has been focussed on phisics informed neural network PINNs \cite{karniadakis2021physics}, \cite{GAO2021110079}, \cite{krishnapriyan2021characterizing}, \cite{roehrl2020modeling}, and on improving modelling of fluids \cite{mahmoudabadbozchelou2021data} \cite{nguyen2022physics}. There has also been a considerable amount of research into converting these discrete step neural networks into continuous depth neural networks, sometimes also referred to as neural ODEs \cite{massaroli2020dissecting} \cite{poli2019graph} \cite{NEURIPS2020_c9f2f917}. Along side this, and given UDEs have gained popularity in the last few years, there have been a number of studies exploring their usefulness in modeling the effect of restrictions due to COVID-19 on the virus' spread \cite{Dandekar2020.04.03.20052084}. Although there has been a large amount of research into the usefulness of these types of models, to the best of this author's knowledge they have never been applied to the problem of predicting temporal social networks. This seems to be a natural fit for UDEs and neural ODEs, given we know relatively little about the processes that govern the evolution of online social networks, and the vast amounts of data that the providers of these services collect, data that can be used to train our neural networks.\\

            
            
        
         Start by illustrating that there is a wide range of applications (Reasonably briefly for not relevant), but little research into UDEs aplicability for modeling how temporal networks behaveb
    \subsection{Dynamical Systems}
    
    \subsection{Symbolic Regression}
        Story is that there is a large search space of potential symbolic equations so most research has been looking to optimise this search. 

        Applications are increadibly varied, as this process is useful for getting interpretable equations from data (with limitations from \cite{kidger2022neural}).

        \cite{kidger2022neural} also demonstrates how to use this with NODEs and their usefulness.

        Examples of implementations include discovering equations governing the movement of simple harmonic and chaotic double pendula from data \cite{schmidt2009distilling}(for the double pendulum searching took between 30-40 hours but when some knowledge of the system was added the time went to 7-8 hours), found underlying differential equations directly from time series data\cite{bongard2007automated}.  

        As well neumerous algorithms exist, with one of the most prominent being genetic programming XXXX research XXXX

    \subsection{Temporal Networks}
        Temporal networks are present in many areas of research interest, ranging from symptom interactions for mental health, to epidemiology, protein interactions, and social networks \cite{jordan2020current,contreras2020temporal,lucas2021inferring,jin2009identifying,masuda2013predicting,moinet2015burstiness,hanneke2010discrete}. With this level of interdisciplinary interest, much of the 
        Wide range of applications (in conference submission)

        challenging problem to model ()


\section{Methods}
    
    \subsection{Embedding Our Network, Singular Value Decomposition}
    We used a singular value decomposition of each matrix to embed our network in a low dimensional space.\
    \[
    A = U\Sigma V'
    \]
    Where $\Sigma_{ii} = \sigma_i$ and $\sigma_i > sigma_{i+1}$ and each $\sigma_i$ is a non-negative square root of the matrices $AA', \space A'A$. $U$ is a matrix made from the orthonormalised eigenvectors of $AA'$ associated with $\sigma_i$, and $V$ is a matrix made from the orthonormalised eigenvectors of $A'A$ associated with $\sigma_i$. Each row $i$ of $U$ and $V$ represents the in and out position of node $i$ respectively. This can be truncated at an arbitrary number of eigenpairs to form a low dimentional approximation to $A$.
    \[
     A \approx \hat U \hat \Sigma \hat V'
    \]\\
    Where $\hat U, \hat \Sigma, \hat V$ are the first $d$ columns of $U, \Sigma, V$ \cite{golub1971singular}. $\hat U \sqrt{\hat \Sigma}$ is still associated with the in edges of our nodes, and $\sqrt{\hat \Sigma} \hat V' $ is still associated with the out edges of our nodes, but now we have simplified our system whilst keeping as much information as possible due to our descending order of singular values. These two matrices form our $d$ dimensional embedding.\\

    The approximation given by this formula can be interpreted as a graph where the entry $(i,j)$ of the matrix are the probability that an edge exists from $i \rightarrow j$.This approach creates a random dot product graph, a class of Latent Position Models \cite{hoff2002latent}. Finding the embedding is not trivial; we need to find the eigenpairs of each of $AA'$ and $A'A$. Because of the computational complexity we use the SVD function in Arpack.jl to approximate each matrix. This then changes the problem of discovering edge formation and destruction into learning a dynamical system.\\
    It is important to note that the SVD is not unique. The distances between points will be the same for every embedding, but the orientation may be different, and so, if the SVD of each time step is not alligned, learning will be effectively impossible [<- show this]. Since our approximation method is an iterative approach, to align each SVD, we can simply pick the same initial vector guess for each [probably worth getting a reference].  
    
    Once we have this embedding of points, we look to model the trajectories of each point. That is, with some input data, point location, distance to neighbours, etc, we want to model the ODE of each point. To do this we make use of the SciML ecosystem of packages in the Julia programming language \cite{SciML_C_Rak}. To explore which architectures perform best for this system we then test various types. ~~~~~~~~~~~~~~~~~~~~~~~~
    
    With our trained NODE, we have a black box that approximates the system that governs the movement of Twitter users. To make this more interpretable we then look to decompose this into linear combinations of known functions using SINDY. For this process we need to define a search space of functions which the algorithm will use to find the best approximation to our NODE with. 
    
    
    
    
    
    
    % We then decide on the number of singular values to take (number of dimensions of our embedding), observing \cite{Runghen2021}, we use [method here including plots]  
    % - Take a time interval of our data and form an undirected network of interactions.
    % - This is not useful for modelling
    % - We embed the network into a low dimensional space where each node becomes a point and nodes that are strongly related become points that are close together. 
    % - This is done using a singular value decomposition (SVD).
    % - 
    % Similar method as Rogini but user Julia's Arpack SVD, not R's \cite{Runghen2021}
    % -->> Estimate adequate dimension using some criteria. \cite{Runghen2021} has references to the criteria they use.
    
    
    
    Initially, we decided the best course of action would be to implement a toy example graph as a proof of concept.
    This would allow us to check that it is indeed possible to use neural networks to model the progession of temporal networks.
    It would also allow us to compare and contrast the use of a "known" function in our model, which will be combined with the 
    neural network to model the evolution of the system over time.
    
    [On Neural Ordinary Differential Equations ch. 2] Potential for starting with a portion of data to minimise getting stuck in 
    local minima.
    \subsection{Testing Different Things}
        Before we begin training our model on the whole data set, we first perform a series of comparisons of different model setups on a subset of our data (1 node 0 knn, 15 nodes 5 knn, 50 nodes 20 knn). Each of these comparisons will be run for ?20? epochs and a ttraining data size of 25 weeks. We will compare both the accuracy and the training time, to develope a model that is likely to perform well on the whole data set. The base model we will be comparing against will have no proposed known component, will have 1 NN with ?2? hidden layers of width 16 each, with an activation function of ?tanh?. The results of this model are
        \begin{table}[]
            \centering
            \begin{tabular}{c|c|c}
                 & training loss & computation time \\
                 \hline
                 super small &  \\
                 small & \\
                 medium &
            \end{tabular}
            \caption{Caption}
            \label{tab:my_label}
        \end{table}
        
    \subsection{Proposed Known Components}
        \subsubsection{Everything Towards Zero} 
        \subsubsection{Including Time Component}
    \subsection{Comparison of Methods on Deterministic Toy Graphs}
        \subsubsection{Toy Graph}
            Toy Graph 1\\
            
            
            Toy Graph 2\\
                Our second toy network was created from a known vector field defined by
                \begin{align}
                    \frac{dx}{dt} &= (x^2+y^2-\frac14)y\\
                    \frac{dy}{dt} &= -(x^2+y^2-\frac14)x
                \end{align}
                With 4 initial points $(x,y)=(0.5,0.5), (0.125, -0.125), (-0.5, 0.0), (-0.5, -0.5)$. One minus the euclidean distance between point $i$ and $j$ is the entry $(i,j)$ in our associated adjacency matrix and corresponds to the probability of an edge existing between these two nodes. Having a network defined by such a system allows us to easily see whether our network (when run through SINDY) is recovering the correct dynamics of the movement of edges between nodes.

            
            
            
            
            
            Explain how the graph is defined and moves over time
        \subsubsection{Tweaks to Method}
            \subsubsection{Aligned vs Unaligned Embeddings}
                %\includesvg[scale=0.5]{plot_aligned_vs_unaligned(draft)}
                How does model accuracy, training time , etc change when the embedding is aligned compared to unaligned? Does alignment method matter?
            \subsubsection{Training on Embedded Data vsThe True Graph}
                Does training the model using the embedded data to calculate loss change the test loss of the model compared to us using the true network matrix?
            \subsubsection{One Neural Network Compared to One For In Edges and One For Out Edges}
            %\includesvg[scale=0.4]{Draft1NNvs2NN}
                Does test error change if we have one NN for the whole graph compared to one for the L and one for the R embeddings? Does it have much impact on computing time? Keep total number of parameters the same (maybe ask Giulio)?
            \subsubsection{Random Noise}
                If we introduce a stochastic component, how does the model cope?
            \subsubsection{Different Dimension Embeddings}
                As we vary the number of dimensions we embed, how does accuracy and computation time change?
            \subsubsection{Training Loss Against Embedding Loss}
                Does the information captured in the SVD embedding correlate with the accuracy of the NN?
            \subsubsection{SINDY}
                If we include a SINDY step, does this improve the accuracy of our model at times far from the training data?
            \subsubsection{trying to predict self loops vs not}
                If we ignore the models prediction of self loops, does this improve the accuracy for the rest of the model?
            \subsection{Including "Known" Differential Equation Component}
                How does the inclusion of the known component effect the accuracy and training time of the model.
                From \cite{garg2009evolution}
                Factors that have an impact on edge generation:
                    
    \subsection{Including Other Information}
        Within the literature, there is evidence of the rate at which a new node creates edges depending on when it was first created, hence it may be worth including this, and other information, (for example the communities a user is in) as an input for the NN.
    \subsection{Variation in the known component}
        \cite{SciML_C_Rak} Demonstrates that if we have a differential equation of the form
        
        \begin{align}
            \dot x &= f_1(x,y)+g_1(x,y) \\
            \dot y &= f_2(x,y)+g_2(x,y)
        \end{align}
        
        ${g_1, \space g_2}$ can be replaced with neural networks $U_1(\theta, x, y), \space U_2(\theta, x, y)$. These neural networks can be trained, and using some type of sparse identification (SINDY), the original $g_1, \space g_2$ can be recovered. This example was given using the Lotka\-Volterra equations in which we knew that $f_1, f_2$ were exactly correct. Given we know very little about the systems of differential equations that govern social networks, we wish to briefly explore what is recovered when the known functions $f_1, \space f_2$ are perturbed to $\hat f_1, \space \hat f_2$. 
\section{Thesis Results}
\subsection{Exploratory Analysis}
- A brief overview of the data following the methodology of \cite{zhao2012multi} and \cite{garg2009evolution}. (Might need to find source code?)
\section{Discussion}

\section{Conclusion}

\printbibliography
\end{document}